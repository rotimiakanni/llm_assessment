# Water Treatment Anomaly Monitoring System

This project simulates real-time sensor data from a water treatment facility, detects anomalies, summarizes them using a local LLM, and exposes a REST API — all running locally via Docker Compose.

---

## Setup Instructions

### Requirements
- Docker and Docker Compose installed
- At least 6GB of RAM available (due to LLM model)
- Python files already organized in service folders:
  - `sensor_generator/`
  - `anomaly_detector/`
  - `llm_summarizer/`
  - `api_server/`

### Quick Start

```bash
git clone https://github.com/rotimiakanni/llm_assessment
cd llm_assessment
```

### Environmen Variables
Make sure to create a .env file in the root directory and add these variables
```bash
SENSOR_ID=wtf-pipe-1
INPUT_FILE=/app/data/sensor_data.jsonl
OUTPUT_FILE=/app/data/anomalies.jsonl
CELERY_BROKER=redis://redis:6379/0
ANOMALY_FILE=/app/data/anomalies.jsonl
SUMMARY_FILE=/app/data/summary.txt
SENSOR_FILE=/app/data/sensor_data.jsonl
```

```bash
docker-compose up --build
```

Optional (run in background):
```bash
docker-compose up -d
```

---

## Detection Thresholds

The anomaly detection system identifies the following types of anomalies based on configured thresholds:

| **Anomaly Type** | **Detection Criteria** |
|------------------|------------------------|
| **Spike**        | A single reading exceeding safe limits:<br>• Temperature > **40°C**<br>• Pressure > **4.0 bar**<br>• Flow > **120 L/min** |
| **Drift**        | Sustained high temperature:<br>• Temperature > **38°C** for more than **15 seconds** |
| **Dropout**      | No sensor readings received:<br>• No data for more than **10 seconds** |

---

## LangChain Integration

The system integrates [LangChain](https://www.langchain.com/) with a local LLM to summarize anomalies in natural language.

- **Model**: [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) (quantized `.gguf` format)
- **Runtime**: Powered by [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python)
- **LangChain Usage**:
  - A prompt template is constructed from the most recent anomaly
  - LangChain runs the model in an offline container
  - The generated summary is saved to `./data/summary.txt`

### ✅ Async Summarization with Celery
- Anomaly detection now enqueues summary tasks via **Celery + Redis**
- Summary generation runs in a separate **`llm_worker`** container
- Decouples LLM load from real-time detection to avoid timeouts or blocking

---

## API Documentation

The system exposes the following REST API via the `api_server` (running on port `8000`):

### `GET /anomalies`
- **Description**: Returns a list of all recent anomalies.
- **Response**: JSON array of anomaly objects from `anomalies.jsonl`.

### `GET /summary`
- **Description**: Returns the most recent summary generated by the LLM.
- **Response**: Plain text string from `summary.txt`.

### `GET /status`
- **Description**: Returns health information about:
  - Sensor activity
  - Anomaly detection
  - Summary generation
- **Response**: JSON object indicating status of each component.

---

## Local Deployment Instructions

You can run the entire system locally using Docker Compose. All services share a common volume (`./data`) for exchanging data.

Make sure to add the environment variables before building the project.

### Build and Run

```bash
docker-compose up --build
```

To run in background:

```bash
docker-compose up -d
```

To stop the system:

```bash
docker-compose down
```

### View logs

```bash
docker-compose logs -f
```

You can also check logs for a specific service:

```bash
docker-compose logs -f anomaly_detector
```

---

## Observability

- Logs are available through Docker Compose (`docker-compose logs`)
- System state is persisted in the shared `./data/` directory:
  - `sensor_data.jsonl` – raw sensor readings
  - `anomalies.jsonl` – structured anomaly records
  - `summary.txt` – latest generated summary

---

## Security Notes

- This system is designed for **offline use** only.
- The LLM (Mistral) runs locally — no API keys or internet access required.
- There is **no authentication** applied to any of the API endpoints.
- All APIs are exposed on `localhost` via Docker and accessible only from the host machine.

---

## Folder Structure

```
project-root/
├── sensor_generator/        # Simulates sensor readings every 2 seconds
│   └── main.py
│
├── anomaly_detector/        # Detects spikes, drift, and dropouts
│   └── main.py
│
├── llm_summarizer/          # LangChain-based summarizer with Mistral LLM
│   ├── summarize.py
│   ├── worker.py            # Celery task handler
│   ├── requirements.txt
│   └── models/              # Contains the mistral .gguf model file
│
├── api_server/              # REST API built with FastAPI
│   ├── main.py
│   └── requirements.txt
│
├── data/                    # Shared folder for sensor logs, anomalies, and summary
│   ├── sensor_data.jsonl
│   ├── anomalies.jsonl
│   └── summary.txt
│
├── docker-compose.yml       # Docker Compose file connecting all services
└── README.md                # Project documentation
```
